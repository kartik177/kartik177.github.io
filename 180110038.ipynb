{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "180110038.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kartik177/kartik177.github.io/blob/master/180110038.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Pn8Qhw8GibD"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import random"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhhfUC1XGibS"
      },
      "source": [
        "class Neural_Network():\n",
        "    def __init__(self, neurons, Activations): \n",
        "        # arguments: an array \"neurons\" consist of number of neurons for each layer, \n",
        "        # an array \"activations\" consisting of activation functions used for the hidden layers and output layer\n",
        "        self.inputSize = neurons[0] # Number of neurons in input layer\n",
        "        self.outputSize = neurons[-1] # Number of neurons in output layer\n",
        "        self.layers = len(neurons)\n",
        "        self.weights = [] #weights for each layer\n",
        "        self.biases = [] #biases in each layer \n",
        "        self.layer_activations = [] #activations in each layer\n",
        "        for i in range(len(neurons)-1): \n",
        "            self.weights.append(np.random.rand(neurons[i+1],neurons[i])) #weight matrix between layer i and layer i+1\n",
        "            self.biases.append(np.random.rand(neurons[i+1],1))\n",
        "            self.layer_activations.append(Activations[i]) #activations for each layer\n",
        "        \n",
        "            \n",
        "    def sigmoid(self, z): # sigmoid activation function\n",
        "        #Fill in the details to compute and return the sigmoid activation function                  \n",
        "        return 1.0/(1.0+np.exp(-z))\n",
        "    \n",
        "    def sigmoidPrime(self,z): # derivative of sigmoid activation function\n",
        "        #Fill in the details to compute and return the derivative of sigmoid activation function\n",
        "        return self.sigmoid(z)*(1-self.sigmoid(z))\n",
        "\n",
        "                          \n",
        "    def tanh(self, z): # hyperbolic tan activation function\n",
        "        #Fill in the details to compute and return the tanh activation function \n",
        "        return (np.exp(z)- np.exp(-z))/(np.exp(z)+np.exp(-z))                 \n",
        "        \n",
        "    \n",
        "    def tanhPrime(self,x): # derivative of hyperbolic tan activation function\n",
        "        #Fill in the details to compute and return the derivative of tanh activation function\n",
        "        return (1 -(self.tanh(x))**2)\n",
        "                          \n",
        "    def linear(self, z): # Linear activation function\n",
        "        #Fill in the details to compute and return the linear activation function                                    \n",
        "        return z\n",
        "    \n",
        "    def linearPrime(self,x): # derivative of linear activation function\n",
        "        #Fill in the details to compute and return the derivative of activation function  \n",
        "        return 1                                                    \n",
        "        if self.ReLU(z) > 0 :\n",
        "            return 1\n",
        "        else:\n",
        "            return 0\n",
        "    \n",
        "\n",
        "    def ReLU(self,z): # ReLU activation function\n",
        "        #Fill in the details to compute and return the ReLU activation function \n",
        "        k = max(z,0)\n",
        "        return k\n",
        "    \n",
        "                      \n",
        "        \n",
        "    \n",
        "    def ReLUPrime(self,z): # derivative of ReLU activation function\n",
        "        #Fill in the details to compute and return the derivative of ReLU activation function\n",
        "        if self.ReLU(z) > 0 :\n",
        "            return 1\n",
        "        else:\n",
        "            return 0                 \n",
        "        \n",
        "    \n",
        "    def forward(self, a): # function of forward pass which will receive input and give the output of final layer\n",
        "        # Write the forward pass using the weights and biases to find the predicted value and return them.\n",
        "        layer_activations_a = [a] #store the input as the input layer activations\n",
        "        layer_dot_prod_z = []\n",
        "        for i, param in enumerate(zip(self.biases, self.weights)):\n",
        "            b, w = param[0], param[1]\n",
        "            if self.layer_activations[i].lower()=='sigmoid':\n",
        "                z = np.dot(w, a)+b\n",
        "                a = self.sigmoid(z)\n",
        "            elif self.layer_activations[i].lower()=='relu':\n",
        "                pass\n",
        "            elif self.layer_activations[i].lower()=='tanh':   \n",
        "                pass\n",
        "            elif self.layer_activations[i].lower()=='linear':\n",
        "                pass\n",
        "            layer_dot_prod_z.append(z)    \n",
        "            layer_activations_a.append(a)\n",
        "        return a, layer_dot_prod_z, layer_activations_a\n",
        "                          \n",
        "            \n",
        "    \n",
        "    def backward(self, x, y, zs, activations): # find the loss and return derivative of loss w.r.t every parameter\n",
        "        # Write the backpropagation algorithm here to find the gradients of weights and biases and return them.\n",
        "        grad_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        grad_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        # b, w 3*1\n",
        "        # backward pass\n",
        "        if self.layer_activations[-1].lower()=='sigmoid':\n",
        "            delta = (activations[-1] - y) *  self.sigmoidPrime(zs[-1])\n",
        "        elif self.layer_activations[-1].lower()=='relu':\n",
        "            delta = (activations[-1] - y) * self.ReLUPrime(zs[-1])\n",
        "        elif self.layer_activations[-1].lower()=='tanh':   \n",
        "            delta = (activations[-1] - y) *  self.tanhPrime(zs[-1])\n",
        "\n",
        "        elif self.layer_activations[-1].lower()=='linear':\n",
        "            delta = (activations[-1] - y) *  self.linearPrime(zs[-1])\n",
        "\n",
        "        grad_b[-1] = delta\n",
        "        grad_w[-1] = delta*(activations[-2].transpose())\n",
        "        delta = delta*((self.weights[-1]).transpose())   \n",
        "        \n",
        "        for l in range(2, self.layers): # Here l is in backward sense i.e. last l th layer\n",
        "            z = zs[-l]\n",
        "            \n",
        "            if self.layer_activations[-l].lower()=='sigmoid':\n",
        "                prime = self.sigmoidPrime(z)\n",
        "            elif self.layer_activations[-l].lower()=='relu':\n",
        "                prime = self.ReLUPrime(z)\n",
        "            elif self.layer_activations[-l].lower()=='tanh':   \n",
        "                prime = self.tanhPrime(z)\n",
        "            elif self.layer_activations[-l].lower()=='linear':\n",
        "                prime = self.linearPrime(z)\n",
        "                \n",
        "\n",
        "            p = np.array(prime[:,0])\n",
        "            #Compute delta, gradients of b and w \n",
        "            \n",
        "           \n",
        "            k =np.matmul((self.weights[-l].transpose()),np.diag(p))\n",
        "            grad_b[-l] = delta\n",
        "            grad_w[-l] = np.matmul((np.diag(p)),delta)\n",
        "            grad_w[-l] = np.matmul(grad_w[-l],(activations[-l-1].transpose()))\n",
        "            delta = np.matmul(k,delta)\n",
        "        return (grad_b, grad_w)    \n",
        "                          \n",
        "                      \n",
        "\n",
        "    def update_parameters(self, grads, learning_rate): # update the parameters using the gradients\n",
        "        # update weights and biases using the gradients and the learning rate\n",
        "        \n",
        "        grad_b, grad_w = grads[0], grads[1]       \n",
        "        \n",
        "        #Implement the update rule for weights  and biases\n",
        "        self.weights = self.weights - np.multiply(learning_rate,grad_w)\n",
        "        self.biases = self.biases - np.multiply(learning_rate,grad_b)\n",
        "        \n",
        "    def loss(self, predicted, actual):\n",
        "        #Implement the loss function\n",
        "        return 0.5*(predicted-actual)**2\n",
        "    def cross_entropy(self, predicted, actual):\n",
        "        KeyboardInterrupt = -actual*np.log(predicted)\n",
        "        return sum(K)   \n",
        "                     \n",
        "    def train(self, X, Y, minibatch=False): # receive the full training data set\n",
        "        lr = 1e-3         # learning rate\n",
        "        epochs = 1000     # number of epochs\n",
        "        loss_list = []\n",
        "        CE_loss_list = []\n",
        "        if minibatch==False:\n",
        "            for e in range(epochs): \n",
        "                losses = []\n",
        "                CE_losses = []\n",
        "                for q in range(len(X)):\n",
        "                    train_x = np.resize(X[q],(X[q].shape[0],1)) \n",
        "                    if not onehotencoded: \n",
        "                        train_y = np.resize(Y[q],(1,1)) \n",
        "                    else:\n",
        "                        train_y = np.resize(np.argmax(Y[q]),(1,1)) \n",
        "                    out, dot_prod_z, activations_a = self.forward(train_x)\n",
        "                    loss = self.loss(out, train_y)\n",
        "                    CE_loss = self.cross_entropy(out,train_y)\n",
        "                    grads = self.backward(train_x, train_y, dot_prod_z, activations_a) # find the gradients using backward pass\n",
        "                    self.update_parameters(grads, lr)\n",
        "                    losses.append(loss)\n",
        "                    CE_losses.append(CE_loss)\n",
        "                loss_list.append(np.mean(np.array(losses)))\n",
        "                CE_loss_list.append(np.mean(np.array(CE_losses)))\n",
        "                print(f'Epoch: {e} Loss: {np.mean(np.array(losses))}')\n",
        "        else:\n",
        "            minibatchsize = 20\n",
        "            create_minibatches(X,Y,minibatchsize)\n",
        "            \n",
        "            for e in range(epochs):\n",
        "                #Complete the training code with minibatches \n",
        "                losses = []\n",
        "                CE_losses = []\n",
        "                minibatch = create_minibatches(X,Y,minibatchsize)\n",
        "                for minibatch in minibatch:\n",
        "                    x_mini,y_mini = minibatch\n",
        "                    x_train = np.resize(x_mini,(minibatchsize, x_mini.shape[1]))\n",
        "                    y_train = []\n",
        "                    for i in range(len(y_mini)):\n",
        "                        if onehotencoded:\n",
        "                            train_y = np.resize(np.argmax(y_mini[i]),(1,1)) \n",
        "                            y_train.append(train_y)\n",
        "                        else:\n",
        "                            pass\n",
        "                        \n",
        "                    output, dot_prod_z ,activations_a = self.forward(x_train)\n",
        "                    loss = self.loss(output,y_train) \n",
        "                    CE_loss = self.cross_entropy(out,train_y)\n",
        "                    grads = self.backward(x_train,y_train, dot_prod_z , activations_a)\n",
        "                    self.update_parameters(grads, lr)\n",
        "                    losses.append(loss)\n",
        "                    CE_losses.append(CE_loss)\n",
        "                loss_list.append(np.mean(np.array(losses)))\n",
        "                CE_loss_list.append(np.mean(np.array(CE_losses)))\n",
        "                print(f'Epoch: {e} Loss: {np.mean(np.array(losses))} CE_Loss: {np.mean(np.array(CE_losses))}')\n",
        "        return loss_list\n",
        "        \n",
        "    def predict(self, x):\n",
        "        print (\"Input : \\n\" + str(x))\n",
        "        prediction,_,_ = self.forward(x)\n",
        "        print (\"Output: \\n\" + str(prediction))\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCnjO3DWGibd"
      },
      "source": [
        "# a method for creating one hot encoded labels \n",
        "def onehotencoding(Y):\n",
        "    Data = sklearn.datasets.load_digits(10,True)\n",
        "    X_data = Data[0]\n",
        "    Y_data = Data[1]\n",
        "    Y = []\n",
        "    for i in len(Y_data):\n",
        "        k = [0]*10\n",
        "        k[Y_data[i]] = 1\n",
        "        Y.append(k)\n",
        "        \n",
        "    return Y\n",
        "\n",
        "#a method to create minibatches \n",
        "def create_minibatches(X,Y,minibatchsize):\n",
        "    numbatches = int(np.ceil(len(X)/minibatchsize))\n",
        "    idx = np.arange(len(X))\n",
        "    np.random.shuffle(idx)\n",
        "    X_minibatches = []\n",
        "    Y_minibatches = [] \n",
        "    for i in range(numbatches):\n",
        "        idx_minibatch = idx[i*minibatchsize:min(len(idx),(i+1)*minibatchsize)]\n",
        "        xn = np.take(X,idx_minibatch,axis=0) \n",
        "        yn = np.take(Y,idx_minibatch,axis=0)\n",
        "        X_minibatches.append(xn)\n",
        "        Y_minibatches.append(yn)\n",
        "    return X_minibatches, Y_minibatches\n",
        "\n",
        "def test_create_minibatches():\n",
        "    X = []\n",
        "    Y = []\n",
        "    inputsize = 3\n",
        "    minibatch = False\n",
        "    onehotencoded = False\n",
        "    n_batch = 20\n",
        "    batch_size = 5\n",
        "    for i in range(50):\n",
        "        if(i % 2 == 0):\n",
        "            X.append([np.random.randint(1,10) for i1 in range(inputsize)])\n",
        "            Y.append(1)\n",
        "        else:\n",
        "            X.append([np.random.randint(-10,1) for i1 in range(inputsize)])\n",
        "            Y.append(0)\n",
        "    X = np.array(X)\n",
        "    Y = np.array(Y)\n",
        "\n",
        "    X_mb, Y_mb = create_minibatches(X,Y,6)\n",
        "    print(X_mb, Y_mb)\n",
        "#test_create_minibatches()"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihJ_JgDoGibp"
      },
      "source": [
        "# Generating some training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrS9yBj5Gibs"
      },
      "source": [
        "X = []\n",
        "Y = []\n",
        "inputsize = 3\n",
        "minibatch = False\n",
        "onehotencoded = False\n",
        "n_batch = 20\n",
        "batch_size = 5\n",
        "for i in range(500):\n",
        "    if(i % 2 == 0):\n",
        "        X.append([random.randint(1,10) for i1 in range(inputsize)])\n",
        "        Y.append(1)\n",
        "    else:\n",
        "        X.append([random.randint(-10,1) for i1 in range(inputsize)])\n",
        "        Y.append(0)\n",
        "X = np.array(X)\n",
        "Y = np.array(Y)\n",
        "if onehotencoded:\n",
        "    Y = onehotencoding(Y)\n",
        "\n",
        "if minibatch==False:\n",
        "    train_X = X\n",
        "    train_Y = Y\n",
        "else:\n",
        "    train_X = []\n",
        "    train_Y = []\n",
        "    for i in range(n_batch):\n",
        "        xn, yn = create_minibatch(X,Y,batch_size)\n",
        "        train_X.append(xn)\n",
        "        train_Y.append(yn)\n",
        "    train_X = np.concatenate(train_X, axis=0).reshape((n_batch,batch_size,inputsize))\n",
        "    train_Y = np.concatenate(train_Y, axis=0).reshape((n_batch,batch_size,-1))"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "rj4G_fcfGicA",
        "outputId": "434ae771-a9e8-4176-d54c-e79ecb926e94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(train_X.shape, train_Y.shape)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(500, 3) (500,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETQPbvbHGicL"
      },
      "source": [
        "# Defining the network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVCmQGmFGicN"
      },
      "source": [
        "#D_in is input dimension\n",
        "#H1 is dimension of first hidden layer \n",
        "#H2 is dimension of second hidden layer\n",
        "#D_out is output dimension.\n",
        "D_in, H1, H2, D_out = inputsize, 10, 5, 1 #You can add more layers if you wish to \n",
        "\n",
        "neurons = [D_in, H1, H2, D_out] # list of number of neurons in the layers sequentially.\n",
        "activation_functions = ['linear','linear','tanh''Relu', 'sigmoid'] #activations in each layer (Note: the input layer does not have any activation)\n",
        "my_neuralnet = Neural_Network(neurons, activation_functions )\n"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjAzA4ieGicZ"
      },
      "source": [
        "# Training the network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "1LaUcEMfGicc",
        "outputId": "5493f2e6-b429-41db-9f6a-ecf973687b86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        }
      },
      "source": [
        "loss = my_neuralnet.train(train_X,train_Y,minibatch=minibatch)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnboundLocalError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-574e3606c7f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_neuralnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_Y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mminibatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mminibatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-32-de55caa3b714>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, Y, minibatch)\u001b[0m\n\u001b[1;32m    160\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m                         \u001b[0mtrain_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m                     \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdot_prod_z\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivations_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                     \u001b[0mCE_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-de55caa3b714>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_activations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'linear'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0mlayer_dot_prod_z\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0mlayer_activations_a\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_dot_prod_z\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_activations_a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'z' referenced before assignment"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NS9Vj2sLGict"
      },
      "source": [
        "# Prediction for a data point after the training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LH05GHEdGicw",
        "outputId": "1f2cc9c2-1909-4395-a404-cb9064e1e25f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "my_neuralnet.predict(np.array([8,4,9]).reshape((3,1)))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input : \n",
            "[[8]\n",
            " [4]\n",
            " [9]]\n",
            "Output: \n",
            "[[0.89162745]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zM0BrWKbGic8",
        "outputId": "2c40eb93-5ad9-43d8-858a-36621d05769d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "plt.plot(loss)\n",
        "plt.show()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJ_MXwG8GidF",
        "outputId": "b2815d76-997f-49f3-ec04-5c7000897d8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn.datasets import load_digits\n",
        "digits = load_digits()\n",
        "\n",
        "\n",
        "print(digits.data.shape)\n",
        "loss"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1797, 64)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.2236634729954681,\n",
              " 0.2224939460621803,\n",
              " 0.22123686943416557,\n",
              " 0.21988364912494643,\n",
              " 0.21842474689063465,\n",
              " 0.21684959079246252,\n",
              " 0.21514648833617236,\n",
              " 0.21330254871765572,\n",
              " 0.21130362399531816,\n",
              " 0.20913428354498853,\n",
              " 0.20677784226132584,\n",
              " 0.2042164709619671,\n",
              " 0.20143142750945867,\n",
              " 0.19840345912963514,\n",
              " 0.19511343939277148,\n",
              " 0.1915433151398298,\n",
              " 0.18767744492567404,\n",
              " 0.1835044038023045,\n",
              " 0.17901929804258856,\n",
              " 0.17422656274281162,\n",
              " 0.16914308988588292,\n",
              " 0.16380134720482142,\n",
              " 0.15825191487597604,\n",
              " 0.1525646454070111,\n",
              " 0.14682755289040433,\n",
              " 0.14114270630138645,\n",
              " 0.1356189471068899,\n",
              " 0.13036213523111104,\n",
              " 0.12546457906728617,\n",
              " 0.12099588037859306,\n",
              " 0.11699724569199854,\n",
              " 0.1134803572824983,\n",
              " 0.11043056709738114,\n",
              " 0.10781307557155159,\n",
              " 0.10558029764627125,\n",
              " 0.10367882360371843,\n",
              " 0.10205497528607381,\n",
              " 0.10065860032585917,\n",
              " 0.09944521906847262,\n",
              " 0.09837688097869358,\n",
              " 0.09742214401432313,\n",
              " 0.09655553958598409,\n",
              " 0.09575679382532998,\n",
              " 0.09500998373065327,\n",
              " 0.09430273249193749,\n",
              " 0.09362549587445,\n",
              " 0.09297095816634533,\n",
              " 0.09233353699290432,\n",
              " 0.09170898659780909,\n",
              " 0.09109408534431182,\n",
              " 0.09048639265675773,\n",
              " 0.08988406177361413,\n",
              " 0.08928569653769973,\n",
              " 0.08869024246072411,\n",
              " 0.08809690418940141,\n",
              " 0.08750508314917982,\n",
              " 0.08691433051626507,\n",
              " 0.08632431178082348,\n",
              " 0.08573478004538108,\n",
              " 0.08514555588986386,\n",
              " 0.0845565121648992,\n",
              " 0.08396756248030707,\n",
              " 0.08337865246345576,\n",
              " 0.08278975309457803,\n",
              " 0.08220085560094588,\n",
              " 0.08161196752283893,\n",
              " 0.0810231096622227,\n",
              " 0.08043431369818362,\n",
              " 0.07984562030767405,\n",
              " 0.07925707767071721,\n",
              " 0.07866874026944637,\n",
              " 0.07808066791285932,\n",
              " 0.07749292493593907,\n",
              " 0.07690557953430087,\n",
              " 0.07631870320487136,\n",
              " 0.07573237027010357,\n",
              " 0.07514665746848397,\n",
              " 0.07456164359804533,\n",
              " 0.07397740920259029,\n",
              " 0.07339403629260327,\n",
              " 0.07281160809456251,\n",
              " 0.07223020882369502,\n",
              " 0.07164992347624721,\n",
              " 0.07107083763814169,\n",
              " 0.07049303730751823,\n",
              " 0.06991660872914893,\n",
              " 0.06934163823911095,\n",
              " 0.06876821211841112,\n",
              " 0.0681964164545114,\n",
              " 0.06762633700990481,\n",
              " 0.06705805909706018,\n",
              " 0.06649166745918697,\n",
              " 0.06592724615638391,\n",
              " 0.06536487845682512,\n",
              " 0.0648046467327123,\n",
              " 0.06424663236078346,\n",
              " 0.06369091562721638,\n",
              " 0.06313757563680891,\n",
              " 0.06258669022634694,\n",
              " 0.06203833588209963,\n",
              " 0.06149258766140084,\n",
              " 0.06094951911829006,\n",
              " 0.060409202233198796,\n",
              " 0.05987170734667521,\n",
              " 0.05933710309714437,\n",
              " 0.058805456362704095,\n",
              " 0.058276832206956286,\n",
              " 0.057751293828871016,\n",
              " 0.05722890251667859,\n",
              " 0.05670971760577893,\n",
              " 0.056193796440652886,\n",
              " 0.05568119434075368,\n",
              " 0.05517196457034942,\n",
              " 0.05466615831228016,\n",
              " 0.054163824645585765,\n",
              " 0.05366501052695314,\n",
              " 0.05316976077592292,\n",
              " 0.052678118063788304,\n",
              " 0.05219012290611198,\n",
              " 0.0517058136587779,\n",
              " 0.05122522651748977,\n",
              " 0.05074839552062018,\n",
              " 0.05027535255530834,\n",
              " 0.04980612736669905,\n",
              " 0.04934074757021001,\n",
              " 0.04887923866670947,\n",
              " 0.04842162406048328,\n",
              " 0.04796792507986538,\n",
              " 0.04751816100040341,\n",
              " 0.04707234907042874,\n",
              " 0.046630504538897596,\n",
              " 0.04619264068536951,\n",
              " 0.04575876885198788,\n",
              " 0.04532889847732623,\n",
              " 0.04490303713196616,\n",
              " 0.044481190555671214,\n",
              " 0.04406336269602326,\n",
              " 0.043649555748388996,\n",
              " 0.043239770197086656,\n",
              " 0.04283400485762466,\n",
              " 0.04243225691988623,\n",
              " 0.04203452199213835,\n",
              " 0.04164079414574534,\n",
              " 0.041251065960471005,\n",
              " 0.04086532857025777,\n",
              " 0.04048357170937399,\n",
              " 0.04010578375882556,\n",
              " 0.03973195179293204,\n",
              " 0.03936206162597151,\n",
              " 0.038996097858802814,\n",
              " 0.038634043925378905,\n",
              " 0.038275882139068455,\n",
              " 0.03792159373870899,\n",
              " 0.03757115893431785,\n",
              " 0.03722455695239263,\n",
              " 0.03688176608073724,\n",
              " 0.03654276371275369,\n",
              " 0.03620752639114511,\n",
              " 0.035876029850978285,\n",
              " 0.03554824906205958,\n",
              " 0.03522415827058169,\n",
              " 0.03490373104000279,\n",
              " 0.034586940291123094,\n",
              " 0.03427375834132799,\n",
              " 0.03396415694297111,\n",
              " 0.03365810732087241,\n",
              " 0.033355580208911675,\n",
              " 0.03305654588570021,\n",
              " 0.03276097420931564,\n",
              " 0.03246883465108935,\n",
              " 0.032180096328437796,\n",
              " 0.03189472803673113,\n",
              " 0.03161269828019624,\n",
              " 0.0313339753018529,\n",
              " 0.031058527112483265,\n",
              " 0.030786321518638773,\n",
              " 0.03051732614968808,\n",
              " 0.03025150848391329,\n",
              " 0.02998883587366254,\n",
              " 0.02972927556956833,\n",
              " 0.02947279474384291,\n",
              " 0.029219360512662693,\n",
              " 0.028968939957655093,\n",
              " 0.028721500146502484,\n",
              " 0.028477008152678132,\n",
              " 0.028235431074330433,\n",
              " 0.027996736052332213,\n",
              " 0.02776089028751264,\n",
              " 0.02752786105708958,\n",
              " 0.027297615730320902,\n",
              " 0.02707012178339342,\n",
              " 0.026845346813569038,\n",
              " 0.026623258552606846,\n",
              " 0.026403824879481144,\n",
              " 0.026187013832414698,\n",
              " 0.025972793620247677,\n",
              " 0.02576113263316099,\n",
              " 0.0255519994527744,\n",
              " 0.025345362861638947,\n",
              " 0.025141191852143148,\n",
              " 0.024939455634852208,\n",
              " 0.024740123646299395,\n",
              " 0.024543165556248737,\n",
              " 0.02434855127444778,\n",
              " 0.0241562509568881,\n",
              " 0.023966235011592917,\n",
              " 0.023778474103948562,\n",
              " 0.023592939161597846,\n",
              " 0.023409601378912327,\n",
              " 0.023228432221060146,\n",
              " 0.023049403427686122,\n",
              " 0.022872487016219904,\n",
              " 0.02269765528482798,\n",
              " 0.022524880815024687,\n",
              " 0.022354136473957304,\n",
              " 0.022185395416379365,\n",
              " 0.02201863108632668,\n",
              " 0.02185381721850928,\n",
              " 0.021690927839432928,\n",
              " 0.021529937268263112,\n",
              " 0.021370820117443488,\n",
              " 0.021213551293081424,\n",
              " 0.021058105995111986,\n",
              " 0.020904459717251973,\n",
              " 0.020752588246754577,\n",
              " 0.02060246766397542,\n",
              " 0.020454074341760276,\n",
              " 0.020307384944664154,\n",
              " 0.020162376428010897,\n",
              " 0.02001902603680315,\n",
              " 0.019877311304490924,\n",
              " 0.019737210051607203,\n",
              " 0.019598700384278984,\n",
              " 0.019461760692621413,\n",
              " 0.01932636964902242,\n",
              " 0.019192506206325313,\n",
              " 0.019060149595915694,\n",
              " 0.0189292793257201,\n",
              " 0.018799875178121813,\n",
              " 0.018671917207800766,\n",
              " 0.018545385739502596,\n",
              " 0.01842026136574274,\n",
              " 0.018296524944451057,\n",
              " 0.018174157596561536,\n",
              " 0.01805314070355248,\n",
              " 0.017933455904941276,\n",
              " 0.01781508509573857,\n",
              " 0.017698010423865693,\n",
              " 0.017582214287539415,\n",
              " 0.017467679332628123,\n",
              " 0.017354388449982525,\n",
              " 0.01724232477274466,\n",
              " 0.01713147167363841,\n",
              " 0.017021812762244463,\n",
              " 0.016913331882262836,\n",
              " 0.016806013108765536,\n",
              " 0.016699840745442198,\n",
              " 0.016594799321841065,\n",
              " 0.016490873590607615,\n",
              " 0.01638804852472314,\n",
              " 0.016286309314745277,\n",
              " 0.01618564136605274,\n",
              " 0.016086030296095573,\n",
              " 0.015987461931653107,\n",
              " 0.01588992230610129,\n",
              " 0.015793397656690526,\n",
              " 0.01569787442183584,\n",
              " 0.01560333923842056,\n",
              " 0.015509778939114838,\n",
              " 0.01541718054971011,\n",
              " 0.01532553128647051,\n",
              " 0.01523481855350265,\n",
              " 0.015145029940144168,\n",
              " 0.0150561532183723,\n",
              " 0.01496817634023321,\n",
              " 0.01488108743529251,\n",
              " 0.014794874808108321,\n",
              " 0.01470952693572675,\n",
              " 0.014625032465200817,\n",
              " 0.01454138021113335,\n",
              " 0.014458559153243943,\n",
              " 0.01437655843396087,\n",
              " 0.014295367356037932,\n",
              " 0.014214975380196957,\n",
              " 0.014135372122795895,\n",
              " 0.014056547353522858,\n",
              " 0.013978490993116481,\n",
              " 0.013901193111112806,\n",
              " 0.013824643923618367,\n",
              " 0.013748833791110276,\n",
              " 0.013673753216262892,\n",
              " 0.013599392841801542,\n",
              " 0.013525743448382841,\n",
              " 0.013452795952502034,\n",
              " 0.013380541404427407,\n",
              " 0.013308970986161258,\n",
              " 0.013238076009427882,\n",
              " 0.013167847913688262,\n",
              " 0.013098278264181496,\n",
              " 0.013029358749992618,\n",
              " 0.01296108118214715,\n",
              " 0.012893437491731654,\n",
              " 0.012826419728040727,\n",
              " 0.012760020056750044,\n",
              " 0.012694230758115004,\n",
              " 0.012629044225195332,\n",
              " 0.01256445296210503,\n",
              " 0.012500449582287722,\n",
              " 0.012437026806816935,\n",
              " 0.012374177462721526,\n",
              " 0.012311894481335661,\n",
              " 0.01225017089667303,\n",
              " 0.012188999843825512,\n",
              " 0.012128374557385796,\n",
              " 0.012068288369893668,\n",
              " 0.012008734710305698,\n",
              " 0.011949707102488329,\n",
              " 0.011891199163733844,\n",
              " 0.011833204603298949,\n",
              " 0.011775717220965973,\n",
              " 0.011718730905626284,\n",
              " 0.01166223963388544,\n",
              " 0.011606237468690172,\n",
              " 0.011550718557976665,\n",
              " 0.011495677133340078,\n",
              " 0.011441107508724844,\n",
              " 0.011387004079135505,\n",
              " 0.011333361319368027,\n",
              " 0.01128017378276103,\n",
              " 0.011227436099966888,\n",
              " 0.011175142977742242,\n",
              " 0.011123289197757924,\n",
              " 0.011071869615427698,\n",
              " 0.011020879158755817,\n",
              " 0.010970312827203009,\n",
              " 0.010920165690570595,\n",
              " 0.010870432887902699,\n",
              " 0.010821109626405935,\n",
              " 0.010772191180386602,\n",
              " 0.010723672890205066,\n",
              " 0.010675550161246926,\n",
              " 0.010627818462910952,\n",
              " 0.010580473327613334,\n",
              " 0.010533510349808171,\n",
              " 0.010486925185023765,\n",
              " 0.010440713548914623,\n",
              " 0.01039487121632895,\n",
              " 0.010349394020391162,\n",
              " 0.010304277851599587,\n",
              " 0.010259518656938573,\n",
              " 0.01021511243900536,\n",
              " 0.010171055255151091,\n",
              " 0.010127343216635769,\n",
              " 0.0100839724877972,\n",
              " 0.010040939285233448,\n",
              " 0.009998239876998586,\n",
              " 0.009955870581811705,\n",
              " 0.009913827768278845,\n",
              " 0.009872107854127631,\n",
              " 0.009830707305454432,\n",
              " 0.009789622635983975,\n",
              " 0.009748850406340834,\n",
              " 0.009708387223333139,\n",
              " 0.00966822973924776,\n",
              " 0.009628374651157231,\n",
              " 0.009588818700237884,\n",
              " 0.009549558671099245,\n",
              " 0.00951059139112433,\n",
              " 0.009471913729820777,\n",
              " 0.009433522598182606,\n",
              " 0.009395414948062404,\n",
              " 0.009357587771553762,\n",
              " 0.009320038100383858,\n",
              " 0.009282763005315883,\n",
              " 0.009245759595561355,\n",
              " 0.00920902501820196,\n",
              " 0.009172556457620757,\n",
              " 0.009136351134942756,\n",
              " 0.00910040630748462,\n",
              " 0.009064719268213334,\n",
              " 0.009029287345213641,\n",
              " 0.008994107901164242,\n",
              " 0.00895917833282238,\n",
              " 0.008924496070516891,\n",
              " 0.008890058577649596,\n",
              " 0.00885586335020454,\n",
              " 0.008821907916265448,\n",
              " 0.008788189835540883,\n",
              " 0.008754706698896981,\n",
              " 0.008721456127898078,\n",
              " 0.0086884357743545,\n",
              " 0.008655643319877677,\n",
              " 0.008623076475442588,\n",
              " 0.008590732980957161,\n",
              " 0.008558610604838687,\n",
              " 0.008526707143596936,\n",
              " 0.008495020421424169,\n",
              " 0.008463548289791543,\n",
              " 0.008432288627052207,\n",
              " 0.008401239338050587]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    }
  ]
}